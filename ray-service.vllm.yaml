apiVersion: ray.io/v1
kind: RayService
metadata:
  name: llama-3-8b
spec:
  serveConfigV2: |
    applications:
    - name: Mistral-7B-Instruct-v0.2
      route_prefix: /Mistral-7B-Instruct-v0.2
      import_path: serve:model
      deployments:
      - name: Mistral-7b-Deployment
        autoscaling_config:
          min_replicas: 1
          max_replicas: 4
        ray_actor_options:
          num_cpus: 25
      runtime_env:
        working_dir: "https://github.com/manikantatarun/kuberay/archive/main.zip"
        pip:
          - vllm==0.6.1.post2
        env_vars:
          MODEL_ID: "/mnt/Mistral-7B-Instruct-v0.2"
          TENSOR_PARALLELISM: "1"
          PIPELINE_PARALLELISM: "1"
          SERVED_MODEL_NAME: "mistral-7b"
          NUMEXPR_MAX_THREADS: "32"
          DEPLOYMENT_NAME: "Mistral-7b-Deployment"
          QUANTIZATION: "awq"
          PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
  rayClusterConfig:
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          affinity: 
            nodeAffinity: 
              requiredDuringSchedulingIgnoredDuringExecution: 
                nodeSelectorTerms: 
                - matchExpressions:
                  - key: purpose 
                    operator: In 
                    values: 
                    - kuberayhead
          containers:
          - name: ray-head
            image: rayproject/ray:2.45.0-py311
          tolerations:
          - key: "kubernetes.azure.com/scalesetpriority"
            operator: "Equal"
            value: "spot"
            effect: "NoSchedule"

            # resources:
            #   limits:
            #     cpu: "2"
            #     memory: "8Gi"
            #   requests:
            #     cpu: "2"
            #     memory: "8Gi"
            # env:
            # - name: HUGGING_FACE_HUB_TOKEN
            #   valueFrom:
            #     secretKeyRef:
            #       name: hf-secret
            #       key: hf_api_token
    workerGroupSpecs:
    - replicas: 1
      minReplicas: 0
      maxReplicas: 4
      groupName: gpu-group
      rayStartParams: {}
      template:
        spec:
          affinity: 
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution: 
                nodeSelectorTerms: 
                - matchExpressions: 
                  - key: purpose
                    operator: In 
                    values: 
                      - kuberay
          securityContext:
            fsGroup: 100
          containers:
          - name: llm
            image: rayproject/ray:2.45.0-py311-gpu
            volumeMounts:
            - name: model-volume
              mountPath: /mnt
            # env:
            # - name: HUGGING_FACE_HUB_TOKEN
            #   valueFrom:
            #     secretKeyRef:
            #       name: hf-secret
            #       key: hf_api_token
            resources:
              limits:
                cpu: "30"
                memory: "300Gi"
                nvidia.com/gpu: "3"
              requests:
                cpu: "30"
                memory: "300Gi"
                nvidia.com/gpu: "3"
          # Please add the following taints to the GPU node.
          volumes:
          - name: model-volume
            persistentVolumeClaim:
              claimName: llms-pvc
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
            - key: "kubernetes.azure.com/scalesetpriority"
              operator: "Equal"
              value: "spot"
              effect: "NoSchedule"
