apiVersion: ray.io/v1
kind: RayService
metadata:
  name: llama-3-8b
spec:
  serveConfigV2: |
    applications:
    - name: llm
      route_prefix: /
      import_path: serve:model
      deployments:
      - name: VLLMDeployment
        num_replicas: 1
        ray_actor_options:
          num_cpus: 32
          # NOTE: num_gpus is set automatically based on TENSOR_PARALLELISM
      runtime_env:
        working_dir: "https://github.com/manikantatarun/kuberay/archive/main.zip"
        pip: ["vllm==0.6.1.post2"]
        env_vars:
          MODEL_ID: "/mnt/Llama-3.2-3B-Instruct"
          TENSOR_PARALLELISM: "1"
          PIPELINE_PARALLELISM: "1"
          NUMEXPR_MAX_THREADS: "32"
  rayClusterConfig:
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          affinity: 
            nodeAffinity: 
              requiredDuringSchedulingIgnoredDuringExecution: 
                nodeSelectorTerms: 
                - matchExpressions:
                  - key: purpose 
                    operator: In 
                    values: 
                    - kuberayhead
          containers:
          - name: ray-head
            image: rayproject/ray:2.45.0-py311
          tolerations:
          - key: "kubernetes.azure.com/scalesetpriority"
            operator: "Equal"
            value: "spot"
            effect: "NoSchedule"

            # resources:
            #   limits:
            #     cpu: "2"
            #     memory: "8Gi"
            #   requests:
            #     cpu: "2"
            #     memory: "8Gi"
            # env:
            # - name: HUGGING_FACE_HUB_TOKEN
            #   valueFrom:
            #     secretKeyRef:
            #       name: hf-secret
            #       key: hf_api_token
    workerGroupSpecs:
    - replicas: 2
      minReplicas: 0
      maxReplicas: 4
      groupName: gpu-group
      rayStartParams: {}
      template:
        spec:
          affinity: 
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution: 
                nodeSelectorTerms: 
                - matchExpressions: 
                  - key: purpose
                    operator: In 
                    values: 
                      - kuberay
          securityContext:
            fsGroup: 100
          containers:
          - name: llm
            image: rayproject/ray:2.45.0-py311-gpu
            volumeMounts:
            - name: model-volume
              mountPath: /mnt
            # env:
            # - name: HUGGING_FACE_HUB_TOKEN
            #   valueFrom:
            #     secretKeyRef:
            #       name: hf-secret
            #       key: hf_api_token
            resources:
              limits:
                cpu: "8"
                memory: "120Gi"
                nvidia.com/gpu: "1"
              requests:
                cpu: "8"
                memory: "120Gi"
                nvidia.com/gpu: "1"
          # Please add the following taints to the GPU node.
          volumes:
          - name: model-volume
            persistentVolumeClaim:
              claimName: llms-pvc
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
            - key: "kubernetes.azure.com/scalesetpriority"
              operator: "Equal"
              value: "spot"
              effect: "NoSchedule"
